{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3c8acf-b2c6-41ab-a0d1-c87bd9b0ff54",
   "metadata": {},
   "source": [
    "# Mobile Robotics Project \n",
    "## Groupe 47\n",
    "\n",
    "| Name      | ID      |\n",
    "|-----------|---------|\n",
    "| Martin    | 340936  |\n",
    "| Laetitia  | 325743  |\n",
    "| Arthur    | 300443  |\n",
    "| Amandine  | 344736  |\n",
    "\n",
    "A **Jupyter notebook** which serves as a report. This must contain the information regarding :\n",
    "- The **members of the group**, it’s helpful to know who you are when we go over the report.\n",
    "- An **introduction to your environment** and to the **choices you made**.\n",
    "- Sections that go into a bit **more detail regarding the implementation** and are accompanied by the code required to **execute the different modules independently**. What is important is not to simply describe the code, which should be readable, but describe what is not in the code: the theory behind, the choices made, the measurements, the choice of parameters etc. Do not forget to **cite your sources** ! You can of course show how certain modules work in **simulation here, or with pre-recorded data**.\n",
    "- A section which is used to **run the overall project** and where we can see the path chosen, where the system believes the robot is along the path before and after filtering etc… This can also be done **in a .py file** if you prefer. Just make sure to reference it in the report so we can easily have a look at it.\n",
    "- Conclusion: The code used to execute the overall project. Do not hesitate to **make use of .py files** that you import in the notebook. The whole body of code does not have to be in the Jupyter notebook, on the contrary! Make sure the **code clean and well documented**. Somebody who starts to browse through it should easily understand what you are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b3905-510c-4c17-b2fd-de0d7d187d78",
   "metadata": {},
   "source": [
    "# Computer vision\n",
    "\n",
    "## ArUco marker\n",
    "### Detecting the bordures\n",
    "### Detecting the Thymio\n",
    "\n",
    "## Detection by color\n",
    "### Detecting the obstacles\n",
    "### Detecting the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d7edf-9f55-438a-af60-dffc5b7987d7",
   "metadata": {},
   "source": [
    "# Kalman filter\n",
    "## Calculating the variances \n",
    "## Applying the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce71db-804b-465f-a963-29f416dc6569",
   "metadata": {},
   "source": [
    "# Path planning\n",
    "## A* algorithm\n",
    "## Finding keypoints of the path\n",
    "For cells every STEP cells, we study the vectors between the previous cell (STEP cells before) and the current cell, and between the current cell and the next cell (STEP cells after).\n",
    "We calculate how much the robot has to turn to change from the first direction to the second one, and if this angle surpasses a threshold (meaning the turn is non negligable), we add the current cell to the list of keypoints of the path. Even if the angles are found insignificant, a cell is added to the keypoints every few steps to make sure we don't accumulate too many small rotations that could become significant when added together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc0b1a2-f685-45d0-81f6-65c5d967c263",
   "metadata": {},
   "source": [
    "# Motion control\n",
    "## Astolfi controller\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817fb7bb-e14a-4c1c-8e69-6501d39325f6",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Using a camera, we detect the scene and do path planning using $A*$\n",
    "\n",
    "1. From camera, capture images at rate $1/f$\n",
    "2. Find edges\n",
    "3. Find corners of edges\n",
    "4. Correct distortion (realign)\n",
    "5. Threshold colors and components size\n",
    "6. Discretize to get the grid\n",
    "7. Compute centroids and enlarged objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1d9ce-69e8-4d87-8eaf-0e06dbc38696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('c:\\\\users\\\\amandine\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.12.0\\\\lib\\\\site-packages')\n",
    "\n",
    "from libs.vision import get_image_from_file, correct_perspective, threshold_image, get_grid\n",
    "from libs.vision import get_centroids, get_nose, get_orientation, grid_to_image, image_to_grid\n",
    "from libs.plot import show_cv2_image, show_perspective, show_thresholds, show_grid, show_nose\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = get_image_from_file(os.path.join(\"robot-env\", \"r2.jpg\")) #calibration\n",
    "show_cv2_image(image, fig_size=(6,6), color=\"BGR\", _title=\"Input from camera\")\n",
    "print(f\"Image shape: {image.shape}, pixels: {image.shape[0]*image.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107149ec-1795-4875-a966-637f3239beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_perspective(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f759d-8d12-466b-8015-3ba233f93a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = correct_perspective(image, sigma=5, epsilon=0.01) # espilon can be large for security\n",
    "show_cv2_image(image, fig_size=(6,6), color=\"BGR\", _title=\"Corrected Perspective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253d95d-535e-437f-a2e9-dc85e582f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_thresholds(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b00f93-4c6e-4792-b8b6-e7f2580aad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# white lower bound (190,190,190) < (b,g,r) < (255,255,255)\n",
    "T_WL=190\n",
    "\n",
    "# (0,0,T_RL) < (b,g,r) < (T_RH,T_RH,255)\n",
    "T_RH=140 # blue green upper bound for red\n",
    "T_RL=120 # red lower bound for red\n",
    "\n",
    "# (0,T_GL,0) < (b,g,r) < (T_GH,255,T_GH)\n",
    "T_GH=140 # blue red upper bound for green\n",
    "T_GL=120 # green lower bound for green\n",
    "\n",
    "min_size=5000\n",
    "\n",
    "image = threshold_image(image, T_WL, T_RH, T_RL, T_GH, T_GL, min_size)\n",
    "show_cv2_image(image, fig_size=(6,6), color=\"BGR\", _title=\"Color and Size Thresholding + Holes filling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c6f8ec-69d5-4162-b84c-db4428ae6f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = get_grid(image, grid_size=200, verbose=True, full_output=False)\n",
    "show_grid(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1e432-78b0-46f4-8e41-9ad8ab81c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_obstacles = get_centroids(grid, \"obstacle\")\n",
    "c_robot = get_centroids(grid, \"start\")\n",
    "c_goal = get_centroids(grid, \"goal\")\n",
    "show_grid(grid, (6,6), c_obstacles, c_robot, c_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94348c16-64d2-457f-9af1-76ac8b9abd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_nose(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0a829-2fbd-4b59-ad75-62fc97537fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nose = get_nose(image, sigma=5, threshold= 50, minLineLength=20, maxLineGap=50)\n",
    "centroid = get_centroids(image, [255,255,255]).flatten()\n",
    "_, angle_deg = get_orientation(nose, centroid)\n",
    "print(nose, centroid, angle_deg) # in image coords, not grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e3bd5-500b-4b08-9665-e2ef294e615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.vision import grid_to_image\n",
    "grid_image = grid_to_image(grid)\n",
    "show_nose(grid_image, sigma_init=5, threshold_init=24, minLineLength_init=20, maxLineGap_init=50, circleSize_init=1)\n",
    "\n",
    "# reduce the threshold to 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dccd990-04d1-4712-af9b-b6e350084429",
   "metadata": {},
   "source": [
    "- Create a big init function, that returns the grid and centroids + nose + orientation (run once at the bigging to setup the hyperparams)\n",
    "\n",
    "- Create an update function to quickly find the robot and update the grid + robot centroid + nose + orientation (camera updates at $1/f$)\n",
    "    - test the time to update (must be fast/real time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52922a8f-6172-4619-b379-4e165a667e90",
   "metadata": {},
   "source": [
    "# Localization Updates\n",
    "\n",
    "We are only interested in the robot\n",
    "\n",
    "Camera helps localization + kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4787fe5-2ae4-4d2f-bf44-25d20db9e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf0e76",
   "metadata": {},
   "source": [
    "### Global function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2dcb24",
   "metadata": {},
   "source": [
    "1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed423277-caf3-4a6d-8c5b-91cd03d085ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync, aw\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "aw(node.lock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56edd035-594d-4341-9183-48384e237620",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from libs.vision import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import feature, measure\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#Variables initialization\n",
    "Thymio_xytheta_hist = np.empty((3, 0)) \n",
    "Thymio_nose_hist = np.empty((2, 0))\n",
    "sigma = 5\n",
    "epsilon = 0.01\n",
    "thresh_Thymio=np.array([180,180,180,255,255,255]) #BGR LLL HHH\n",
    "thresh_obstacle=np.array([[60,65,180,110,110,255]]) #40,30,170,100,100,240\n",
    "thresh_goal=np.array([80,130,100,115,255,140])\n",
    "min_size=5000\n",
    "grid_size=200\n",
    "Thymio_detected=False\n",
    "\n",
    "print(\"Try to open camera\")\n",
    "\n",
    "login=\"thymio\"\n",
    "password=\"thymio\"\n",
    "url = f\"https://{login}:{password}@10.160.89.100:8080/video\" # you can check the url @ http://192.168.1.14:8080/\n",
    "cam = cv2.VideoCapture(url)\n",
    "\n",
    "#cam = cv2.VideoCapture(1, cv2.CAP_DSHOW) #Specify DirectShow for faster connection\n",
    "if not cam.isOpened(): \n",
    "    print(\"Camera could not be opened\") \n",
    "    cam.release()\n",
    "    exit()\n",
    "\n",
    "# Initialize OpenCV window and matplotlib plot\n",
    "cv2.namedWindow(\"Camera view\", cv2.WND_PROP_FULLSCREEN)\n",
    "cv2.imshow(\"Camera view\", get_image_from_camera(cam,False)) #to be removed if not using goodimg.png for testing!\n",
    "\n",
    "cv2.waitKey(1)#to be removed if not using goodimg.png for testing!\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(3)\n",
    "\n",
    "\n",
    "image, grid, Thymio_xytheta, c_goal, path, Thymio_detected, Thymio_nose,obstacle_cnt, obstacle_cnt_expnded, goal_cnt, Thymio_cnt, Thymio_size, Thymio_nose, mat_persp,max_width_persp, max_height_persp=init(cam, sigma, epsilon, thresh_Thymio,thresh_obstacle, thresh_goal, min_size, grid_size)\n",
    "\n",
    "#path is in grid coord need to be in im coord for plot\n",
    "path_img= grid1_coord2grid2_coord(path,grid,image)\n",
    "path_img=path_img[::-1]\n",
    "\n",
    "image_cnt=draw_cnt_image(image,goal_cnt,obstacle_cnt,obstacle_cnt_expnded,Thymio_cnt,path_img,Thymio_xytheta,Thymio_nose,c_goal)\n",
    "\n",
    "#Update history\n",
    "Thymio_xytheta_hist=np.hstack((Thymio_xytheta_hist,Thymio_xytheta))\n",
    "Thymio_nose_hist=np.hstack((Thymio_nose_hist,Thymio_nose))\n",
    "\n",
    "#Initial Image and Plots:\n",
    "cv2.imshow(\"Camera view\", image_cnt)\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].clear()\n",
    "    ax[i].plot(np.arange(Thymio_xytheta_hist.shape[1]),Thymio_xytheta_hist[i,:],'b-',label='Vision')\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel(\"Step\")\n",
    "    if i==0:\n",
    "        ax[i].set_ylabel(\"X (pixels)\")\n",
    "    elif i==1:\n",
    "        ax[i].set_ylabel(\"Y (pixels)\")\n",
    "    elif i==2:\n",
    "        ax[i].set_ylabel(\"Theta (rad)\")\n",
    "    \n",
    "\n",
    "plt.draw()\n",
    "cv2.waitKey(500)\n",
    "\n",
    "show_grid(grid)\n",
    "\n",
    "#cam.release()\n",
    "#cv2.destroyAllWindows()\n",
    "######################################################\n",
    "#UPDATE\n",
    "######################################################\n",
    "from motion_control.motion import motion_control\n",
    "import time\n",
    "x_goal=(c_goal.flatten())[0]\n",
    "y_goal=(c_goal.flatten())[1]\n",
    "\n",
    "\n",
    "for steps in range(20):\n",
    "    #print(steps)\n",
    "    #print(\"before update camera\")\n",
    "    image, Thymio_xytheta, Thymio_detected, Thymio_size, Thymio_nose, Thymio_cnt=update_vision(cam, sigma, epsilon, mat_persp,max_width_persp, max_height_persp, thresh_Thymio, Thymio_size)\n",
    "    #print(\"after update camera\")\n",
    "    \n",
    "    #MOTION CONTROL :\n",
    "    time.sleep(0.1)\n",
    "    print(\"step \",steps)\n",
    "    x=(Thymio_xytheta.flatten())[0]\n",
    "    y=(Thymio_xytheta.flatten())[1]\n",
    "    if(steps==0) :\n",
    "        print(\"first position :\", x,\" \", y)\n",
    "        print(\"goal position :\",x_goal,y_goal)\n",
    "    theta = (Thymio_xytheta.flatten())[2]\n",
    "    #print(\"before motion control\")\n",
    "    #v_m = motion_control(x,y,theta,x_goal,y_goal)\n",
    "    v_m = motion_control(x,y,theta,x_goal,y_goal)\n",
    "    print(\"v_m = \", v_m)\n",
    "    await node.set_variables(v_m)\n",
    "    \n",
    "    image_cnt=draw_cnt_image(image,goal_cnt,obstacle_cnt,obstacle_cnt_expnded,Thymio_cnt,path_img,Thymio_xytheta,Thymio_nose,c_goal)\n",
    "\n",
    "    #Update history\n",
    "    Thymio_xytheta_hist=np.hstack((Thymio_xytheta_hist,Thymio_xytheta))\n",
    "    Thymio_nose_hist=np.hstack((Thymio_nose_hist,Thymio_nose))\n",
    "    #Initial Image and Plots:\n",
    "    cv2.imshow(\"Camera view\", image_cnt)\n",
    "    cv2.waitKey(1)\n",
    "    for i in range(3):\n",
    "        ax[i].clear()\n",
    "        ax[i].plot(np.arange(Thymio_xytheta_hist.shape[1]),Thymio_xytheta_hist[i,:],'b-',label='Vision')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_xlabel(\"Step\")\n",
    "        if i==0:\n",
    "            ax[i].set_ylabel(\"X (pixels)\")\n",
    "        elif i==1:\n",
    "            ax[i].set_ylabel(\"Y (pixels)\")\n",
    "        elif i==2:\n",
    "            ax[i].set_ylabel(\"Theta (rad)\")\n",
    "        \n",
    "    plt.draw()\n",
    "    cv2.waitKey(10)\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a818f-33ba-463d-80a4-2e3b93fc83c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the program\n",
    "aw(node.stop())\n",
    "aw(node.unlock())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f5fab",
   "metadata": {},
   "source": [
    "2. Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c53284",
   "metadata": {},
   "outputs": [],
   "source": [
    "for steps in range(30):\n",
    "\n",
    "    image, Thymio_xytheta, Thymio_detected, Thymio_size, Thymio_nose, Thymio_cnt=update_vision(cam, sigma, epsilon, thresh_Thymio, Thymio_size)\n",
    "    \n",
    "    image_cnt=draw_cnt_image(image,goal_cnt,obstacle_cnt,obstacle_cnt_expnded,Thymio_cnt,path_img,Thymio_xytheta,Thymio_nose,c_goal)\n",
    "\n",
    "\n",
    "    #Update history\n",
    "    Thymio_xytheta_hist=np.hstack((Thymio_xytheta_hist,Thymio_xytheta))\n",
    "    Thymio_nose_hist=np.hstack((Thymio_nose_hist,Thymio_nose))\n",
    "\n",
    "    #Initial Image and Plots:\n",
    "    cv2.imshow(\"Camera view\", image_cnt)\n",
    "    cv2.waitKey(500)\n",
    "    for i in range(3):\n",
    "        ax[i].clear()\n",
    "        ax[i].plot(np.arange(Thymio_xytheta_hist.shape[1]),Thymio_xytheta_hist[i,:],'b-',label='Vision')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_xlabel(\"Step\")\n",
    "        if i==0:\n",
    "            ax[i].set_ylabel(\"X (pixels)\")\n",
    "        elif i==1:\n",
    "            ax[i].set_ylabel(\"Y (pixels)\")\n",
    "        elif i==2:\n",
    "            ax[i].set_ylabel(\"Theta (rad)\")\n",
    "        \n",
    "    plt.draw()\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b95da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from libs.vision import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import feature, measure\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "#Variables initialization\n",
    "Thymio_xytheta_hist=[]\n",
    "Thymio_nose_hist=[]\n",
    "sigma = 5\n",
    "epsilon = 0.01\n",
    "thresh_Thymio=np.array([190,190,190,255,255,255])\n",
    "thresh_obstacle=np.array([[0,0,120,0,0,140]])\n",
    "thresh_goal=np.array([0,120,0,0,140,0])\n",
    "min_size=5000\n",
    "grid_size=200\n",
    "Thymio_detected=False\n",
    "\n",
    "print(\"Try to open camera\")\n",
    "\n",
    "login=\"thymio\"\n",
    "password=\"qwertz\"\n",
    "url = f\"http://{login}:{password}@192.168.21.126:8080/video\" # you can check the url @ http://192.168.1.14:8080/\n",
    "cam = cv2.VideoCapture(url)\n",
    "#cam = cv2.VideoCapture(1, cv2.CAP_DSHOW) #Specify DirectShow for faster connection\n",
    "if not cam.isOpened(): \n",
    "    print(\"Camera could not be opened\") \n",
    "    cam.release()\n",
    "    exit()\n",
    "\n",
    "# Initialize OpenCV window and matplotlib plot\n",
    "cv2.namedWindow(\"Camera view\", cv2.WINDOW_NORMAL)\n",
    "#cv2.imshow(\"Camera view\", get_image_from_camera(cam))\n",
    "while True:\n",
    "    # Grab a frame from the stream\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Camera view\", frame)\n",
    "\n",
    "    # Exit loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.waitKey(20000)\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730a205",
   "metadata": {},
   "source": [
    "2. Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22b4d8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
