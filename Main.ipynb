{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3c8acf-b2c6-41ab-a0d1-c87bd9b0ff54",
   "metadata": {},
   "source": [
    "# Mobile Robotics Project \n",
    "## Groupe 47\n",
    "\n",
    "| Name      | SCIPER  |\n",
    "|-----------|---------|\n",
    "| Martin    | 340936  |\n",
    "| Laetitia  | 325743  |\n",
    "| Arthur    | 300443  |\n",
    "| Amandine  | 344736  |\n",
    "\n",
    "A **Jupyter notebook** which serves as a report. This must contain the information regarding :\n",
    "- The **members of the group**, it’s helpful to know who you are when we go over the report.\n",
    "- An **introduction to your environment** and to the **choices you made**.\n",
    "- Sections that go into a bit **more detail regarding the implementation** and are accompanied by the code required to **execute the different modules independently**. What is important is not to simply describe the code, which should be readable, but describe what is not in the code: the theory behind, the choices made, the measurements, the choice of parameters etc. Do not forget to **cite your sources** ! You can of course show how certain modules work in **simulation here, or with pre-recorded data**.\n",
    "- A section which is used to **run the overall project** and where we can see the path chosen, where the system believes the robot is along the path before and after filtering etc… This can also be done **in a .py file** if you prefer. Just make sure to reference it in the report so we can easily have a look at it.\n",
    "- Conclusion: The code used to execute the overall project. Do not hesitate to **make use of .py files** that you import in the notebook. The whole body of code does not have to be in the Jupyter notebook, on the contrary! Make sure the **code clean and well documented**. Somebody who starts to browse through it should easily understand what you are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017a0d7",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "As part of the course Basics of mobile robotics, given to master students at EPFL by professor Mondada, we did a project using a Thymio and a camera. First, with computer vision, we map the environment where the Thymio will evolve, creating a grid based on the limits of the workspace and mapping global obstacles and the goal the robot has to reach. We localize the Thymio and find the shortest path he can follow to reach the goal without encountering any global obstacles. The Thymio then follows this path and can react to local obstcles added in front of him, avoiding them and continuing towards his goal. \n",
    "\n",
    "To do : describe the environment and the scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b3905-510c-4c17-b2fd-de0d7d187d78",
   "metadata": {},
   "source": [
    "# Computer vision\n",
    "\n",
    "## ArUco marker\n",
    "### Detecting the bordures\n",
    "### Detecting the Thymio\n",
    "\n",
    "## Detection by color\n",
    "### Detecting the obstacles\n",
    "### Detecting the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d7edf-9f55-438a-af60-dffc5b7987d7",
   "metadata": {},
   "source": [
    "# Kalman filter\n",
    "## Calculating the variances \n",
    "## Applying the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce71db-804b-465f-a963-29f416dc6569",
   "metadata": {},
   "source": [
    "# Path planning\n",
    "## A* algorithm\n",
    "## Finding keypoints of the path\n",
    "\n",
    "Given the size of the Thymio compared to the size of the grid cells, we won't follow the path cell by cell but instead extract keypoints that will serve as intermediate goals for the Thymio.\n",
    "For cells every STEP cells, we study the vectors between the previous cell (STEP cells before) and the current cell, and between the current cell and the next cell (STEP cells after).\n",
    "We calculate how much the robot has to turn to change from the first direction to the second one, and if this angle surpasses a threshold (meaning the turn is non negligable), we add the current cell to the list of keypoints of the path. Even if the angles are found insignificant, a cell is added to the keypoints every few steps to make sure we don't ignore too many small changes of direction that could become significant when added together.\n",
    "\n",
    "In this algorithm we chose to study the path in steps of size STEP instead of considering every single one of the cells because the cells of the grid are very small compared to the Thymio. \n",
    "\n",
    "Using math.atan2(det, dot_product) allows us to find the signed angle between the two vectors. atan2() can find angles over the whole trigonometric circle (using the information given by the sign of the dot_product), not limiting itself to only two quadrants like atan()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a6d26",
   "metadata": {},
   "source": [
    "# Motion control\n",
    "## Astolfi controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceefd70",
   "metadata": {},
   "source": [
    "Diagram showing the thymio, the goal, and the variables used for the astolfi controller :\n",
    "\n",
    "<img src=\"schema_astolfi.jpg\" alt=\"Diagram\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc0b1a2-f685-45d0-81f6-65c5d967c263",
   "metadata": {},
   "source": [
    "We use an Astolfi controller to make the robot move towards his goal. We chose this kind of controller because it makes the Thymio moves more smoothly than advancing segment by segment and making the robot turn on itself between each segment. Indeed it makes a mix of both the wanted translational and rotational velocities to set the motors' speed :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6da952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snippet of code from the function motion_control(x,y,theta,x_goal,y_goal) in motion.py :\n",
    "\n",
    "v = k_rho*distance_to_goal #translational velocity\n",
    "omega = k_alpha*(delta_angle) - k_beta*(delta_angle+theta) #rotational velocity\n",
    "    \n",
    "#Calculate motor speed\n",
    "w_ml = (v+omega*L)//R_WHEEL #rad/s \n",
    "w_mr = (v-omega*L)//R_WHEEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5330935",
   "metadata": {},
   "source": [
    "The translational and rotational velocities depend on the distance and angle to the goal, multiplied by the controllers parameters.\n",
    "We tuned the controllers parameters by testing. k_rho controls the translational velocity, k_alpha the rotational velocity, and k_beta is a damping term (to stabilize the robot's orientation when reaching the goal).\n",
    "\n",
    "We obtain motors' speed in rad/s, so before setting it to the thymio we scale it to fit the pwm, limit it (the thymio's motor can accept values ranging from -500 to 500), and convert them to integers (type accepted by the thymio)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817fb7bb-e14a-4c1c-8e69-6501d39325f6",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Using a camera, we detect the scene and do path planning using $A*$\n",
    "\n",
    "1. From camera, capture images at rate $1/f$\n",
    "2. Find edges\n",
    "3. Find corners of edges\n",
    "4. Correct distortion (realign)\n",
    "5. Threshold colors and components size\n",
    "6. Discretize to get the grid\n",
    "7. Compute centroids and enlarged objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1d9ce-69e8-4d87-8eaf-0e06dbc38696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('c:\\\\users\\\\amandine\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.12.0\\\\lib\\\\site-packages')\n",
    "\n",
    "from libs.vision import get_image_from_file, correct_perspective, threshold_image, get_grid\n",
    "from libs.vision import get_centroids, get_nose, get_orientation, grid_to_image, image_to_grid\n",
    "from libs.plot import show_cv2_image, show_perspective, show_thresholds, show_grid, show_nose\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = get_image_from_file(os.path.join(\"robot-env\", \"r2.jpg\")) #calibration\n",
    "show_cv2_image(image, fig_size=(6,6), color=\"BGR\", _title=\"Input from camera\")\n",
    "print(f\"Image shape: {image.shape}, pixels: {image.shape[0]*image.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107149ec-1795-4875-a966-637f3239beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_perspective(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f759d-8d12-466b-8015-3ba233f93a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = correct_perspective(image, sigma=5, epsilon=0.01) # espilon can be large for security\n",
    "show_cv2_image(image, fig_size=(6,6), color=\"BGR\", _title=\"Corrected Perspective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253d95d-535e-437f-a2e9-dc85e582f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_thresholds(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b00f93-4c6e-4792-b8b6-e7f2580aad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# white lower bound (190,190,190) < (b,g,r) < (255,255,255)\n",
    "T_WL=190\n",
    "\n",
    "# (0,0,T_RL) < (b,g,r) < (T_RH,T_RH,255)\n",
    "T_RH=140 # blue green upper bound for red\n",
    "T_RL=120 # red lower bound for red\n",
    "\n",
    "# (0,T_GL,0) < (b,g,r) < (T_GH,255,T_GH)\n",
    "T_GH=140 # blue red upper bound for green\n",
    "T_GL=120 # green lower bound for green\n",
    "\n",
    "min_size=5000\n",
    "\n",
    "image = threshold_image(image, T_WL, T_RH, T_RL, T_GH, T_GL, min_size)\n",
    "show_cv2_image(image, fig_size=(6,6), color=\"BGR\", _title=\"Color and Size Thresholding + Holes filling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c6f8ec-69d5-4162-b84c-db4428ae6f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = get_grid(image, grid_size=200, verbose=True, full_output=False)\n",
    "show_grid(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1e432-78b0-46f4-8e41-9ad8ab81c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_obstacles = get_centroids(grid, \"obstacle\")\n",
    "c_robot = get_centroids(grid, \"start\")\n",
    "c_goal = get_centroids(grid, \"goal\")\n",
    "show_grid(grid, (6,6), c_obstacles, c_robot, c_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94348c16-64d2-457f-9af1-76ac8b9abd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_nose(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0a829-2fbd-4b59-ad75-62fc97537fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nose = get_nose(image, sigma=5, threshold= 50, minLineLength=20, maxLineGap=50)\n",
    "centroid = get_centroids(image, [255,255,255]).flatten()\n",
    "_, angle_deg = get_orientation(nose, centroid)\n",
    "print(nose, centroid, angle_deg) # in image coords, not grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e3bd5-500b-4b08-9665-e2ef294e615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.vision import grid_to_image\n",
    "grid_image = grid_to_image(grid)\n",
    "show_nose(grid_image, sigma_init=5, threshold_init=24, minLineLength_init=20, maxLineGap_init=50, circleSize_init=1)\n",
    "\n",
    "# reduce the threshold to 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dccd990-04d1-4712-af9b-b6e350084429",
   "metadata": {},
   "source": [
    "- Create a big init function, that returns the grid and centroids + nose + orientation (run once at the bigging to setup the hyperparams)\n",
    "\n",
    "- Create an update function to quickly find the robot and update the grid + robot centroid + nose + orientation (camera updates at $1/f$)\n",
    "    - test the time to update (must be fast/real time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52922a8f-6172-4619-b379-4e165a667e90",
   "metadata": {},
   "source": [
    "# Localization Updates\n",
    "\n",
    "We are only interested in the robot\n",
    "\n",
    "Camera helps localization + kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4787fe5-2ae4-4d2f-bf44-25d20db9e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf0e76",
   "metadata": {},
   "source": [
    "### Global function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2dcb24",
   "metadata": {},
   "source": [
    "1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed423277-caf3-4a6d-8c5b-91cd03d085ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync, aw\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "aw(node.lock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56edd035-594d-4341-9183-48384e237620",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from libs.vision import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import feature, measure\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#Variables initialization\n",
    "Thymio_xytheta_hist = np.empty((3, 0)) \n",
    "Thymio_nose_hist = np.empty((2, 0))\n",
    "sigma = 5\n",
    "epsilon = 0.01\n",
    "thresh_Thymio=np.array([180,180,180,255,255,255]) #BGR LLL HHH\n",
    "thresh_obstacle=np.array([[60,65,180,110,110,255]]) #40,30,170,100,100,240\n",
    "thresh_goal=np.array([80,130,100,115,255,140])\n",
    "min_size=5000\n",
    "grid_size=200\n",
    "Thymio_detected=False\n",
    "\n",
    "print(\"Try to open camera\")\n",
    "\n",
    "login=\"thymio\"\n",
    "password=\"thymio\"\n",
    "url = f\"https://{login}:{password}@10.160.89.100:8080/video\" # you can check the url @ http://192.168.1.14:8080/\n",
    "cam = cv2.VideoCapture(url)\n",
    "\n",
    "#cam = cv2.VideoCapture(1, cv2.CAP_DSHOW) #Specify DirectShow for faster connection\n",
    "if not cam.isOpened(): \n",
    "    print(\"Camera could not be opened\") \n",
    "    cam.release()\n",
    "    exit()\n",
    "\n",
    "# Initialize OpenCV window and matplotlib plot\n",
    "cv2.namedWindow(\"Camera view\", cv2.WND_PROP_FULLSCREEN)\n",
    "cv2.imshow(\"Camera view\", get_image_from_camera(cam,False)) #to be removed if not using goodimg.png for testing!\n",
    "\n",
    "cv2.waitKey(1)#to be removed if not using goodimg.png for testing!\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(3)\n",
    "\n",
    "\n",
    "image, grid, Thymio_xytheta, c_goal, path, Thymio_detected, Thymio_nose,obstacle_cnt, obstacle_cnt_expnded, goal_cnt, Thymio_cnt, Thymio_size, Thymio_nose, mat_persp,max_width_persp, max_height_persp=init(cam, sigma, epsilon, thresh_Thymio,thresh_obstacle, thresh_goal, min_size, grid_size)\n",
    "\n",
    "#path is in grid coord need to be in im coord for plot\n",
    "path_img= grid1_coord2grid2_coord(path,grid,image)\n",
    "path_img=path_img[::-1]\n",
    "\n",
    "image_cnt=draw_cnt_image(image,goal_cnt,obstacle_cnt,obstacle_cnt_expnded,Thymio_cnt,path_img,Thymio_xytheta,Thymio_nose,c_goal)\n",
    "\n",
    "#Update history\n",
    "Thymio_xytheta_hist=np.hstack((Thymio_xytheta_hist,Thymio_xytheta))\n",
    "Thymio_nose_hist=np.hstack((Thymio_nose_hist,Thymio_nose))\n",
    "\n",
    "#Initial Image and Plots:\n",
    "cv2.imshow(\"Camera view\", image_cnt)\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].clear()\n",
    "    ax[i].plot(np.arange(Thymio_xytheta_hist.shape[1]),Thymio_xytheta_hist[i,:],'b-',label='Vision')\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel(\"Step\")\n",
    "    if i==0:\n",
    "        ax[i].set_ylabel(\"X (pixels)\")\n",
    "    elif i==1:\n",
    "        ax[i].set_ylabel(\"Y (pixels)\")\n",
    "    elif i==2:\n",
    "        ax[i].set_ylabel(\"Theta (rad)\")\n",
    "    \n",
    "\n",
    "plt.draw()\n",
    "cv2.waitKey(500)\n",
    "\n",
    "show_grid(grid)\n",
    "\n",
    "#cam.release()\n",
    "#cv2.destroyAllWindows()\n",
    "######################################################\n",
    "#UPDATE\n",
    "######################################################\n",
    "from motion_control.motion import motion_control\n",
    "import time\n",
    "x_goal=(c_goal.flatten())[0]\n",
    "y_goal=(c_goal.flatten())[1]\n",
    "\n",
    "\n",
    "for steps in range(20):\n",
    "    #print(steps)\n",
    "    #print(\"before update camera\")\n",
    "    image, Thymio_xytheta, Thymio_detected, Thymio_size, Thymio_nose, Thymio_cnt=update_vision(cam, sigma, epsilon, mat_persp,max_width_persp, max_height_persp, thresh_Thymio, Thymio_size)\n",
    "    #print(\"after update camera\")\n",
    "    \n",
    "    #MOTION CONTROL :\n",
    "    time.sleep(0.1)\n",
    "    print(\"step \",steps)\n",
    "    x=(Thymio_xytheta.flatten())[0]\n",
    "    y=(Thymio_xytheta.flatten())[1]\n",
    "    if(steps==0) :\n",
    "        print(\"first position :\", x,\" \", y)\n",
    "        print(\"goal position :\",x_goal,y_goal)\n",
    "    theta = (Thymio_xytheta.flatten())[2]\n",
    "    #print(\"before motion control\")\n",
    "    #v_m = motion_control(x,y,theta,x_goal,y_goal)\n",
    "    v_m = motion_control(x,y,theta,x_goal,y_goal)\n",
    "    print(\"v_m = \", v_m)\n",
    "    await node.set_variables(v_m)\n",
    "    \n",
    "    image_cnt=draw_cnt_image(image,goal_cnt,obstacle_cnt,obstacle_cnt_expnded,Thymio_cnt,path_img,Thymio_xytheta,Thymio_nose,c_goal)\n",
    "\n",
    "    #Update history\n",
    "    Thymio_xytheta_hist=np.hstack((Thymio_xytheta_hist,Thymio_xytheta))\n",
    "    Thymio_nose_hist=np.hstack((Thymio_nose_hist,Thymio_nose))\n",
    "    #Initial Image and Plots:\n",
    "    cv2.imshow(\"Camera view\", image_cnt)\n",
    "    cv2.waitKey(1)\n",
    "    for i in range(3):\n",
    "        ax[i].clear()\n",
    "        ax[i].plot(np.arange(Thymio_xytheta_hist.shape[1]),Thymio_xytheta_hist[i,:],'b-',label='Vision')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_xlabel(\"Step\")\n",
    "        if i==0:\n",
    "            ax[i].set_ylabel(\"X (pixels)\")\n",
    "        elif i==1:\n",
    "            ax[i].set_ylabel(\"Y (pixels)\")\n",
    "        elif i==2:\n",
    "            ax[i].set_ylabel(\"Theta (rad)\")\n",
    "        \n",
    "    plt.draw()\n",
    "    cv2.waitKey(10)\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a818f-33ba-463d-80a4-2e3b93fc83c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the program\n",
    "aw(node.stop())\n",
    "aw(node.unlock())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f5fab",
   "metadata": {},
   "source": [
    "2. Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c53284",
   "metadata": {},
   "outputs": [],
   "source": [
    "for steps in range(30):\n",
    "\n",
    "    image, Thymio_xytheta, Thymio_detected, Thymio_size, Thymio_nose, Thymio_cnt=update_vision(cam, sigma, epsilon, thresh_Thymio, Thymio_size)\n",
    "    \n",
    "    image_cnt=draw_cnt_image(image,goal_cnt,obstacle_cnt,obstacle_cnt_expnded,Thymio_cnt,path_img,Thymio_xytheta,Thymio_nose,c_goal)\n",
    "\n",
    "\n",
    "    #Update history\n",
    "    Thymio_xytheta_hist=np.hstack((Thymio_xytheta_hist,Thymio_xytheta))\n",
    "    Thymio_nose_hist=np.hstack((Thymio_nose_hist,Thymio_nose))\n",
    "\n",
    "    #Initial Image and Plots:\n",
    "    cv2.imshow(\"Camera view\", image_cnt)\n",
    "    cv2.waitKey(500)\n",
    "    for i in range(3):\n",
    "        ax[i].clear()\n",
    "        ax[i].plot(np.arange(Thymio_xytheta_hist.shape[1]),Thymio_xytheta_hist[i,:],'b-',label='Vision')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_xlabel(\"Step\")\n",
    "        if i==0:\n",
    "            ax[i].set_ylabel(\"X (pixels)\")\n",
    "        elif i==1:\n",
    "            ax[i].set_ylabel(\"Y (pixels)\")\n",
    "        elif i==2:\n",
    "            ax[i].set_ylabel(\"Theta (rad)\")\n",
    "        \n",
    "    plt.draw()\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b95da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from libs.vision import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import feature, measure\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "#Variables initialization\n",
    "Thymio_xytheta_hist=[]\n",
    "Thymio_nose_hist=[]\n",
    "sigma = 5\n",
    "epsilon = 0.01\n",
    "thresh_Thymio=np.array([190,190,190,255,255,255])\n",
    "thresh_obstacle=np.array([[0,0,120,0,0,140]])\n",
    "thresh_goal=np.array([0,120,0,0,140,0])\n",
    "min_size=5000\n",
    "grid_size=200\n",
    "Thymio_detected=False\n",
    "\n",
    "print(\"Try to open camera\")\n",
    "\n",
    "login=\"thymio\"\n",
    "password=\"qwertz\"\n",
    "url = f\"http://{login}:{password}@192.168.21.126:8080/video\" # you can check the url @ http://192.168.1.14:8080/\n",
    "cam = cv2.VideoCapture(url)\n",
    "#cam = cv2.VideoCapture(1, cv2.CAP_DSHOW) #Specify DirectShow for faster connection\n",
    "if not cam.isOpened(): \n",
    "    print(\"Camera could not be opened\") \n",
    "    cam.release()\n",
    "    exit()\n",
    "\n",
    "# Initialize OpenCV window and matplotlib plot\n",
    "cv2.namedWindow(\"Camera view\", cv2.WINDOW_NORMAL)\n",
    "#cv2.imshow(\"Camera view\", get_image_from_camera(cam))\n",
    "while True:\n",
    "    # Grab a frame from the stream\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Camera view\", frame)\n",
    "\n",
    "    # Exit loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.waitKey(20000)\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730a205",
   "metadata": {},
   "source": [
    "2. Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22b4d8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
