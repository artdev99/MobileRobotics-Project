{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobile Robotics Project \n",
    "## Groupe 47\n",
    "\n",
    "| Name      | SCIPER  |\n",
    "|-----------|---------|\n",
    "| Martin    | 340936  |\n",
    "| Laetitia  | 325743  |\n",
    "| Arthur    | 300443  |\n",
    "| Amandine  | 344736  |\n",
    "\n",
    "A **Jupyter notebook** which serves as a report. This must contain the information regarding :\n",
    "- The **members of the group**, it’s helpful to know who you are when we go over the report.\n",
    "- An **introduction to your environment** and to the **choices you made**.\n",
    "- Sections that go into a bit **more detail regarding the implementation** and are accompanied by the code required to **execute the different modules independently**. What is important is not to simply describe the code, which should be readable, but describe what is not in the code: the theory behind, the choices made, the measurements, the choice of parameters etc. Do not forget to **cite your sources** ! You can of course show how certain modules work in **simulation here, or with pre-recorded data**.\n",
    "- A section which is used to **run the overall project** and where we can see the path chosen, where the system believes the robot is along the path before and after filtering etc… This can also be done **in a .py file** if you prefer. Just make sure to reference it in the report so we can easily have a look at it.\n",
    "- Conclusion: The code used to execute the overall project. Do not hesitate to **make use of .py files** that you import in the notebook. The whole body of code does not have to be in the Jupyter notebook, on the contrary! Make sure the **code clean and well documented**. Somebody who starts to browse through it should easily understand what you are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "As part of the course Basics of mobile robotics, given to master students at EPFL by professor Mondada, we did a project using a Thymio and a camera. First, with computer vision, we map the environment where the Thymio will evolve, creating a grid based on the limits of the workspace and mapping global obstacles and the goal the robot has to reach. We localize the Thymio and find the shortest path he can follow to reach the goal without encountering any global obstacles. The Thymio then follows this path and can react to local obstcles added in front of him, avoiding them and continuing towards his goal. \n",
    "\n",
    "In our project the robot is moving on a white A0 sheet for better contrasts and flatness with 4 aruco markers on the borders to better correct the perspective. The Thymio has an aruco markers on him for the detection. The global obstacles are red paper sheets to make arbitrary shapes and move them around easily. For the same reasons, the goal is a green paper sheet. The local obstacles are ???."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "- White A0 sheet as background\n",
    "- Goal: a green shape\n",
    "- Global obstacles: red shapes\n",
    "- Local obstacles (invisible to camera): circular shape of diameter $\\approx 5cm$\n",
    "- Aukey PC-W1 Webcam 1080p\n",
    "- Wireless Thymio with Aruco {id:9, size:70mm}\n",
    "- Top Left ArUco: {id:0, size:70mm}\n",
    "- Bottom Left ArUco: {id:1, size:70mm}\n",
    "- Top Right ArUco: {id:10, size:70mm}\n",
    "- Bototm Right ArUco: {id:2, size:70mm}\n",
    "\n",
    "![placeholder for scene]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "Here is a flow chart of the code structure. The code is divided in 4 files. The main file following the flow chart, the Thymio class containing the thymio's information, the thymio's detection, the kalman filter and the two controllers (Astolfi and local avoidance), the camera class containing the camera warmup, image capture, corner detection, perspective correction, obstacle and goal detection and plotting. The 4th file is for the path planning (A*, key points image to grid conversion)\n",
    "We will then go through these features:\n",
    "1. Computer vision : corners, Thymio, obstacles and goal detections\n",
    "2. Path Planning : A* Algorithm, key points\n",
    "3. Kalman Filter : Motion equations and covariances measurements\n",
    "4. Motion control : Astolfi Controller\n",
    "5. Local Avoidance : ???\n",
    "\n",
    "And finally we will see how they are integrated in the main programm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Computer vision\n",
    "This section will present the computer vision implementation. We used two different detection methods: The aruco markers and the color detection.\n",
    "\n",
    "Because the camera (Aukey PC-W1 Webcamera 1080p) has an Automatic White Balance and Automatic Exposure Adjustment, the first frames appear yellowish. To give the camera time to adapt to the scene, we take 50 images :\n",
    "```python\n",
    "for _ in range(50):  # Camera needs to do lighting and white balance adjustments\n",
    "    ret, _ = self.cam.read()\n",
    "    if not ret:\n",
    "        self.cam.release()\n",
    "        raise IOError(\"Failed to capture frame during warmup\")\n",
    "```\n",
    "\n",
    "### 1.a ArUco Markers\n",
    "We choose aruco markers for detecting the corners of the arena (the environement containing the robot, obstacles and goal) and the thymio itself, because they are robust to varying lighting conditions and provide precise detection. We used the original aruco dictionnary as it proved to be more robust to us after trying multiple dictionnaries. \n",
    "We choose to use the OpenCV aruco detector module for a more straightforward,efficient and robust approach. The steps of the detection can be found in [the documentation](https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html) and [here](https://mmla.gse.harvard.edu/tools/js-aruco-marker-detector/). To get the markers, we used the [ArUco markers generator!](https://chev.me/arucogen/) by Oleg Kalachev. We chose the markers to have a size of $70mm$ and ids: $[0,1,2,10]$ for the arena and $9$ for the Thymio.\n",
    "\n",
    "#### 1.a.i Corner detection and perspective\n",
    "`find_aruco_corners_size()`\n",
    "After getting the aruco markers in the corners of the arena we want to get a conversion factor from pixels to mm to later convert the Thymio's position in pixel to mm for the Kalman filter and controller. We simply take the mean of all 16 sides of the markers and divide it by the real length in mm:\n",
    "```python\n",
    "size_aruco=[]\n",
    "for i in range(4):\n",
    "    side_lengths = [\n",
    "        np.linalg.norm(corners[i][0,0,:] - corners[i][0,1,:]),  # Top side\n",
    "        np.linalg.norm(corners[i][0,1,:] - corners[i][0,2,:]),  # Right side\n",
    "        np.linalg.norm(corners[i][0,2,:] - corners[i][0,3,:]),  # Bottom side\n",
    "        np.linalg.norm(corners[i][0,3,:] - corners[i][0,0,:])   # Left side\n",
    "    ]\n",
    "    size_aruco.append(np.mean(side_lengths))\n",
    "return np.array(outer_corners), np.mean(size_aruco)\n",
    "```\n",
    "Having the corners of the A0 sheet we use `cv2.warpPerspective()` to get a flatter image for a better visualisation and better detection this will correct the \"trapezoidal\" deformation because our camera is not exactly above the A0 sheet and centered. And to have the edges of the A0 sheet (x and y axis of our coordinate system) parallel to the image frame so that we can directly convert a pixel coordinate to our real world coordinate (A0 sheet).\n",
    "\n",
    "#### 1.a.ii Thymio detection\n",
    "`Thymio_position_aruco()`\n",
    "The Thymio's position is found again with OpenCV's aruco marker detector.\n",
    "The angle is found by taking the mean of the angle of the top and bottom edge of the marker w.r.t. the x axis:\n",
    "```python\n",
    "        if (ids is None) or (self.Thymio_ID not in ids):\n",
    "            self.Thymio_detected=False\n",
    "        else:\n",
    "            idx = np.where(ids == self.Thymio_ID)[0][0] #Thymio's aruco ID is 9\n",
    "            aruco_corners=np.array(corners[idx][0,:,:])\n",
    "\n",
    "            #Thymio's center:\n",
    "            Thymio_x,Thymio_y=aruco_corners.mean(axis=0)\n",
    "\n",
    "            #Thymio's angle\n",
    "            top_edge=aruco_corners[1,:]-aruco_corners[0,:]\n",
    "            bottom_edge=aruco_corners[2,:]-aruco_corners[3,:]\n",
    "            angle = np.mean([np.arctan2(bottom_edge[1], bottom_edge[0]), \n",
    "                            np.arctan2(top_edge[1], top_edge[0])])\n",
    "\n",
    "            self.xytheta_meas = np.array([Thymio_x,Thymio_y,angle])\n",
    "            self.Thymio_detected=True\n",
    "```\n",
    "#### 1.b Color detection\n",
    "The detection of the obstacle and goal is done in multiple steps in `full_detection_cnt_centroid()`:\n",
    "1. We create a binary mask with color thresholding. We pick the treshold by looking at the min max of the BGR values on [an online color picker](https://imagecolorpicker.com/). Because we only detect 2 colors that are fairly distinct we could relax our treshold to have a robust system.\n",
    "2. We take the biggest contour for the goal because there is only 1 green object. The other might be noise or thymio leds And for the obstacles we remove blobs smaller than a certain area that is likely to be noise. The threshold has been found through tests.\n",
    "3. We fill the contour to remove holes to have a nicer plot and calculate the center of the goal by calculating the centroids.\n",
    "5. We expand the contours by the radius of the thymio (70mm that we convert to pixels with our conversion factor) by taking all pixels that are at a certain distance from the obstacle. Because we use the L2 norm as heuristic in the path planning we use the same distance here\n",
    "\n",
    "```python\n",
    "distance = cv2.distanceTransform(~obstacle_mask, cv2.DIST_L2, 5)\n",
    "# Expand the obstacle by Thymio's radius\n",
    "expanded_obstacle_mask = (distance < radius) * 255\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Path Planning\n",
    "### 2.a A*\n",
    "We choose the A star algorithm for path planning because:\n",
    "- it is complete and will always give us a solution\n",
    "- an image is like a grid so the preprocessing to get the grid is straightforward\n",
    "- we can choose it's precision/running time by tuning the grid size: it is only done at the beginning (where we don't care about time) and at after each local avoidance so we can have a precise planning at the beginning and a faster after local avoidance\n",
    "The downsides are:\n",
    "- We choose to consider 8 neighbors and the L2 norm as heuristic because the robot can go diagonnaly. However, the robot is not limited to 45 degrees angles so the soltuion is not optimal for the robot\n",
    "- we need to extract keypoints to give more latitude to the controller to o to each of them and to avoid too many changes:if the optimal is to go 30 degrees down right the path will be a lot of variations between going straight/down/45 degrees. With key points the robot only need to go from the beginning and end of the slope and could do 30 degrees directly optimizing a bit.\n",
    "\n",
    "The implemenation of the A* is quite similar to the exercice session except a $\\sqrt{2}$ to account for a longer distance when doing a diagonal move:\n",
    "```python\n",
    "if (neighbor[0]==current_pos[0]) or (neighbor[1]==current_pos[1]): #if goes straight\n",
    "    tentative_g_cost = g_costs[current_pos]+(map_grid[neighbor]) #cost is 1 y default on the map_grid\n",
    "else:\n",
    "    tentative_g_cost = g_costs[current_pos]+(map_grid[neighbor])*np.sqrt(2) #going diagonnaly is going further\n",
    "\n",
    "```\n",
    "\n",
    "### 2.b Key points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman filter\n",
    "## Calculating the variances \n",
    "## Applying the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path planning\n",
    "## A* algorithm\n",
    "## Finding keypoints of the path\n",
    "\n",
    "Given the size of the Thymio compared to the size of the grid cells, we won't follow the path cell by cell but instead extract keypoints that will serve as intermediate goals for the Thymio.\n",
    "For cells every STEP cells, we study the vectors between the previous cell (STEP cells before) and the current cell, and between the current cell and the next cell (STEP cells after).\n",
    "We calculate how much the robot has to turn to change from the first direction to the second one, and if this angle surpasses a threshold (meaning the turn is non negligable), we add the current cell to the list of keypoints of the path. Even if the angles are found insignificant, a cell is added to the keypoints every few steps to make sure we don't ignore too many small changes of direction that could become significant when added together.\n",
    "\n",
    "In this algorithm we chose to study the path in steps of size STEP instead of considering every single one of the cells because the cells of the grid are very small compared to the Thymio. \n",
    "\n",
    "Using math.atan2(det, dot_product) allows us to find the signed angle between the two vectors. atan2() can find angles over the whole trigonometric circle (using the information given by the sign of the dot_product), not limiting itself to only two quadrants like atan()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion control\n",
    "## Astolfi controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagram showing the thymio, the goal, and the variables used for the astolfi controller :\n",
    "\n",
    "<img src=\"./images/schema_astolfi.jpg\" alt=\"Diagram\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an Astolfi controller to make the robot move towards his goal. We chose this kind of controller because it makes the Thymio moves more smoothly than advancing segment by segment and making the robot turn on itself between each segment. Indeed it makes a mix of both the wanted translational and rotational velocities to set the motors' speed :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snippet of code from the function motion_control(x,y,theta,x_goal,y_goal) in motion.py :\n",
    "\n",
    "v = k_rho*distance_to_goal                                  #translational velocity [cm/s]\n",
    "omega = k_alpha*(delta_angle) - k_beta*(delta_angle+theta)  #rotational velocity [rad/s]\n",
    "   \n",
    "#Calculate motor speed\n",
    "w_ml = (v+omega*L_AXIS)/R_WHEEL #[rad/s]\n",
    "w_mr = (v-omega*L_AXIS)/R_WHEEL #[rad/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The translational and rotational velocities depend on the distance and angle to the goal, multiplied by the controllers parameters.\n",
    "We tuned the controllers parameters by testing. k_rho controls the translational velocity, k_alpha the rotational velocity, and k_beta is a damping term (to stabilize the robot's orientation when reaching the goal).\n",
    "\n",
    "We obtain motors' speed in rad/s, so before setting it to the thymio we scale it to fit the pwm, limit it (the thymio's motor can accept values ranging from -500 to 500), and convert them to integers (type accepted by the thymio). \n",
    "To find the scaling factor between the motors' speed in rad/s and the motors' target, we used the information given in the thymio cheat sheet : when the motors are set at 500, the translational velocity is equal to about 20cm/s. Therefore we obtain : \n",
    "\n",
    "SCALING_FACTOR = 500/(20*10^-2/R_WHEEL)\n",
    "\n",
    "(using R_WHEEL to convert m/s to rad/s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local obstacle avoidance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync, aw\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "aw(node.lock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import numpy as np\n",
    "from libs.vision import *\n",
    "from skimage import feature, measure\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "from heapq import heappush, heappop\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Variables initialization\n",
    "Thymio_xytheta_hist = np.empty((0, 3))\n",
    "sigma = 5\n",
    "epsilon = 0.01\n",
    "thresh_obstacle = np.array([[4, 0, 90, 90, 90, 255]])  # Adjust thresholds if needed\n",
    "thresh_goal = np.array([30, 30, 20, 90, 120, 80])\n",
    "min_size = 500\n",
    "grid_size = 300\n",
    "Thymio_detected = False\n",
    "\n",
    "# Uncomment and adjust if using an IP camera\n",
    "\"\"\"\n",
    "print(\"Try to open camera\")\n",
    "\n",
    "login = \"thymio\"\n",
    "password = \"qwertz\"\n",
    "url = f\"https://{login}:{password}@192.168.21.126:8080/video\"  # Check the URL if needed\n",
    "cam = cv2.VideoCapture(url)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Try to open camera\")\n",
    "\n",
    "cam = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "\n",
    "if not cam.isOpened():\n",
    "    print(\"Camera could not be opened\")\n",
    "    cam.release()\n",
    "    exit()\n",
    "\n",
    "print(\"Warming up the camera...\")\n",
    "for _ in range(50):  # Number of frames to discard (adjust if needed)\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame\")\n",
    "        break\n",
    "\n",
    "# Initialization function\n",
    "image, grid, Thymio_xytheta, c_goal, path, obstacle_cnt, obstacle_cnt_expnded, goal_cnt, \\\n",
    "mat_persp, max_width_persp, max_height_persp, aruco_size = init(\n",
    "    cam, sigma, epsilon, thresh_obstacle, thresh_goal, min_size, grid_size, aruco=True)\n",
    "\n",
    "# Convert path coordinates for plotting\n",
    "path_img = grid1_coord2grid2_coord(path, grid, image)\n",
    "path_img = path_img[::-1]\n",
    "\n",
    "image_cnt = draw_cnt_image(\n",
    "    image, goal_cnt, obstacle_cnt, obstacle_cnt_expnded, path_img, Thymio_xytheta, c_goal, aruco_size,Thymio_detected)\n",
    "\n",
    "# Update history\n",
    "Thymio_xytheta_hist = np.vstack((Thymio_xytheta_hist, Thymio_xytheta))\n",
    "\n",
    "print(f\"Initialization time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "######################################################\n",
    "# UPDATE\n",
    "######################################################\n",
    "from motion_control.motion import *\n",
    "\n",
    "for steps in range(500):\n",
    "    start_time = time.time()\n",
    "    image, Thymio_xytheta, Thymio_detected = update_vision(\n",
    "        cam, sigma, epsilon, mat_persp, max_width_persp, max_height_persp)\n",
    "    \n",
    "    #motion_control(adjust_units(Thymio_xytheta, c_goal, aruco_size))\n",
    "\n",
    "    if not Thymio_detected:\n",
    "        Thymio_xytheta = Thymio_xytheta_hist[-1, :]\n",
    "\n",
    "    image_cnt = draw_cnt_image(\n",
    "        image, goal_cnt, obstacle_cnt, obstacle_cnt_expnded, path_img, Thymio_xytheta, c_goal, aruco_size,Thymio_detected)\n",
    "\n",
    "    # Update history\n",
    "    Thymio_xytheta_hist = np.vstack((Thymio_xytheta_hist, Thymio_xytheta))\n",
    "\n",
    "    # Display the image and plots using cv2.imshow\n",
    "    cv2.imshow('Camera View', image_cnt)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    print(f\"Time for this step: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Clean up\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the program\n",
    "aw(node.stop())\n",
    "aw(node.unlock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
