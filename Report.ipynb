{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobile Robotics Project \n",
    "## Groupe 47\n",
    "\n",
    "| Name      | SCIPER  |\n",
    "|-----------|---------|\n",
    "| Martin    | 340936  |\n",
    "| Laetitia  | 325743  |\n",
    "| Arthur    | 300443  |\n",
    "| Amandine  | 344736  |\n",
    "\n",
    "A **Jupyter notebook** which serves as a report. This must contain the information regarding :\n",
    "- The **members of the group**, it’s helpful to know who you are when we go over the report.\n",
    "- An **introduction to your environment** and to the **choices you made**.\n",
    "- Sections that go into a bit **more detail regarding the implementation** and are accompanied by the code required to **execute the different modules independently**. What is important is not to simply describe the code, which should be readable, but describe what is not in the code: the theory behind, the choices made, the measurements, the choice of parameters etc. Do not forget to **cite your sources** ! You can of course show how certain modules work in **simulation here, or with pre-recorded data**.\n",
    "- A section which is used to **run the overall project** and where we can see the path chosen, where the system believes the robot is along the path before and after filtering etc… This can also be done **in a .py file** if you prefer. Just make sure to reference it in the report so we can easily have a look at it.\n",
    "- Conclusion: The code used to execute the overall project. Do not hesitate to **make use of .py files** that you import in the notebook. The whole body of code does not have to be in the Jupyter notebook, on the contrary! Make sure the **code clean and well documented**. Somebody who starts to browse through it should easily understand what you are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "As part of the course Basics of mobile robotics, given to master students at EPFL by professor Mondada, we did a project using a Thymio and a camera. First, with computer vision, we map the environment where the Thymio will evolve, creating a grid based on the limits of the workspace and mapping global obstacles and the goal the robot has to reach. We localize the Thymio and find the shortest path he can follow to reach the goal without encountering any global obstacles. The Thymio then follows this path and can react to local obstcles added in front of him, avoiding them and continuing towards his goal. \n",
    "\n",
    "In our project the robot is moving on a white A0 sheet for better contrasts and flatness with 4 aruco markers on the borders to better correct the perspective. The Thymio has an aruco markers on him for the detection. The global obstacles are red paper sheets to make arbitrary shapes and move them around easily. For the same reasons, the goal is a green paper sheet. The local obstacles are represented as 5cm circular paprika container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Arena\n",
    "- Background: a white A0 sheet\n",
    "- Goal: a green shape\n",
    "- Global obstacles: red shapes\n",
    "- Local obstacles (invisible to camera): circular object of diameter $\\approx 5cm$, like a paprika container\n",
    "- Aukey PC-W1 Webcam 1080p\n",
    "- Wireless Thymio with Aruco {id:9, size:70mm}\n",
    "- Top Left ArUco: {id:0, size:70mm}\n",
    "- Bottom Left ArUco: {id:1, size:70mm}\n",
    "- Top Right ArUco: {id:10, size:70mm}\n",
    "- Bottom Right ArUco: {id:2, size:70mm}\n",
    "\n",
    "<img src=\"https://i.postimg.cc/CLs2grqx/image-2024-12-04-145126355.png\" alt=\"env setup\" width=\"500\">\n",
    "\n",
    "Note: the aruco markers placement order and orientation is very important for computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "Here is a flow chart of the code structure. The code is divided in 4 files. The main file following the flow chart, the Thymio class containing the thymio's information, the thymio's detection, the kalman filter and the two controllers (Astolfi and local avoidance), the camera class containing the camera warmup, image capture, arena orner detection, perspective correction, obstacle and goal detection and plotting. The 4th file is for the path planning (A*, key points image to grid conversion)\n",
    "We will then go through these features:\n",
    "1. Computer vision : arena corners, Thymio, obstacles and goal detections, obstacles expansion\n",
    "2. Path Planning : A* Algorithm, key points\n",
    "3. Kalman Filter : Motion equations and covariances measurements\n",
    "4. Motion control : Astolfi Controller\n",
    "5. Local Avoidance : Braitenberg triangles\n",
    "\n",
    "And finally we will see how they are integrated in the main programm.\n",
    "\n",
    "<img src=\"https://i.postimg.cc/yNG1T81t/image-2024-12-04-151247433.png\" alt=\"flowchart\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Computer vision\n",
    "This section will present the computer vision implementation. We used two different detection methods: The aruco markers and the color detection.\n",
    "\n",
    "As the camera has automatic white balance (AWB) and automatic exposure (AE), the very first frames appear yellowish. To give the camera time to adapt to the scene, we take 50 images:\n",
    "```python\n",
    "for _ in range(50):  # Camera needs to do lighting and white balance adjustments\n",
    "    ret, _ = self.cam.read()\n",
    "    if not ret:\n",
    "        self.cam.release()\n",
    "        raise IOError(\"Failed to capture frame during warmup\")\n",
    "```\n",
    "\n",
    "### 1.a ArUco Markers\n",
    "We choose aruco markers for detecting the corners of the arena (the environement containing the robot, obstacles and goal) and the thymio itself, because they are robust to varying lighting conditions and provide precise detection. We used the original aruco dictionnary as it proved to be more robust to us after trying multiple dictionnaries. \n",
    "We choose to use the OpenCV aruco detector module for a more straightforward,efficient and robust approach. The steps of the detection can be found in [the documentation](https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html) and [here](https://mmla.gse.harvard.edu/tools/js-aruco-marker-detector/). To get the markers, we used the [ArUco markers generator!](https://chev.me/arucogen/) by Oleg Kalachev. We chose the markers to have a size of $70mm$ and ids: $[0,1,2,10]$ for the arena and $9$ for the Thymio.\n",
    "\n",
    "#### 1.a.i Corner detection and perspective\n",
    "`find_aruco_corners_size()`\n",
    "After getting the aruco markers in the corners of the arena we want to get a conversion factor from pixels to mm to later convert the Thymio's position in pixel to mm for the Kalman filter and controller. We simply take the mean of all 16 sides of the markers and divide it by the real length in mm:\n",
    "```python\n",
    "size_aruco_px=[]\n",
    "for i in range(4):\n",
    "    side_lengths = [\n",
    "        np.linalg.norm(corners[i][0,0,:] - corners[i][0,1,:]),  # Top side\n",
    "        np.linalg.norm(corners[i][0,1,:] - corners[i][0,2,:]),  # Right side\n",
    "        np.linalg.norm(corners[i][0,2,:] - corners[i][0,3,:]),  # Bottom side\n",
    "        np.linalg.norm(corners[i][0,3,:] - corners[i][0,0,:])   # Left side\n",
    "    ]\n",
    "    size_aruco_px.append(np.mean(side_lengths))\n",
    "return np.array(outer_corners), np.mean(size_aruco_px)\n",
    "```\n",
    "Having the corners of the A0 sheet, we find the maximum width and height of the area defined by the 4 corners, and then compute the perspective transformation matrix $M$, that maps from the distroted space to the new perspective corrected space. Finally, we apply this transformation by using `cv2.warpPerspective()` to isolate our arena, which allows better visualisation and detection. This is necessary, as our camera is not exactly above the A0 sheet and perfectly centered (\"trapezoidal\" deformation), and it allows having the edges of the A0 sheet (x and y axis of our coordinate system) parallel to the edges of the corrected image frame (pixel coordinate system), aswell as to find the convertion ratio from mm to pixel, needed later for Kalman.\n",
    "\n",
    "#### 1.a.ii Thymio detection\n",
    "`Thymio_position_aruco()`\n",
    "Similarly, the Thymio's position is found with OpenCV's aruco marker detector.\n",
    "The angle is found by taking the mean of the angle of the top and bottom edge of the marker w.r.t. the x axis, allowing to efficiently find the $(x,y,\\theta)$ of the robot:\n",
    "```python\n",
    "        if (ids is None) or (self.Thymio_ID not in ids):\n",
    "            self.Thymio_detected=False\n",
    "        else:\n",
    "            idx = np.where(ids == self.Thymio_ID)[0][0] #Thymio's aruco ID is 9\n",
    "            aruco_corners=np.array(corners[idx][0,:,:])\n",
    "\n",
    "            #Thymio's center:\n",
    "            Thymio_x,Thymio_y=aruco_corners.mean(axis=0)\n",
    "\n",
    "            #Thymio's angle\n",
    "            top_edge=aruco_corners[1,:]-aruco_corners[0,:]\n",
    "            bottom_edge=aruco_corners[2,:]-aruco_corners[3,:]\n",
    "            angle = np.mean([np.arctan2(bottom_edge[1], bottom_edge[0]), \n",
    "                            np.arctan2(top_edge[1], top_edge[0])])\n",
    "\n",
    "            self.xytheta_meas = np.array([Thymio_x,Thymio_y,angle])\n",
    "            self.Thymio_detected=True\n",
    "```\n",
    "#### 1.b Color detection\n",
    "The detection of the obstacle and goal is done in multiple steps in `full_detection_cnt_centroid()`:\n",
    "1. We create a binary mask with color thresholding. We pick the treshold by looking at the min max of the BGR values on [an online color picker](https://imagecolorpicker.com/). Because we only detect 2 colors that are fairly distinct we could relax our treshold to have a robust system.\n",
    "2. We take the biggest contour for the goal because there is only 1 green object. The other might be noise or thymio leds And for the obstacles we remove blobs smaller than a certain area that is likely to be noise. The threshold has been found through tests.\n",
    "3. We fill the contour to remove holes to have a nicer plot and calculate the center of the goal by calculating the centroids.\n",
    "5. We expand the contours by the radius of the thymio (70mm that we convert to pixels with our conversion factor) by taking all pixels that are at a certain distance from the obstacle. Because we use the L2 norm as heuristic in the path planning we use the same distance here\n",
    "\n",
    "```python\n",
    "distance = cv2.distanceTransform(~obstacle_mask, cv2.DIST_L2, 5)\n",
    "# Expand the obstacle by Thymio's radius\n",
    "expanded_obstacle_mask = (distance < radius) * 255\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Path Planning\n",
    "### 2.a A*\n",
    "We choose the A star algorithm for path planning because:\n",
    "- it is complete and will always give us a solution\n",
    "- an image is like a grid so the preprocessing to get the grid is straightforward\n",
    "- we can choose it's precision/running time by tuning the grid size: it is only done at the beginning (where we don't care about time) and at after each local avoidance so we can have a precise planning at the beginning and a faster after local avoidance\n",
    "The downsides are:\n",
    "- We choose to consider 8 neighbors and the L2 norm as heuristic because the robot can go diagonnaly. However, the robot is not limited to 45 degrees angles so the soltuion is not optimal for the robot\n",
    "- we need to extract keypoints to give more latitude to the controller to o to each of them and to avoid too many changes:if the optimal is to go 30 degrees down right the path will be a lot of variations between going straight/down/45 degrees. With key points the robot only need to go from the beginning and end of the slope and could do 30 degrees directly optimizing a bit.\n",
    "\n",
    "The implemenation of the A* is quite similar to the exercice session except a $\\sqrt{2}$ to account for a longer distance when doing a diagonal move:\n",
    "```python\n",
    "if (neighbor[0]==current_pos[0]) or (neighbor[1]==current_pos[1]): #if goes straight\n",
    "    tentative_g_cost = g_costs[current_pos]+(map_grid[neighbor]) #cost is 1 y default on the map_grid\n",
    "else:\n",
    "    tentative_g_cost = g_costs[current_pos]+(map_grid[neighbor])*np.sqrt(2) #going diagonnaly is going further\n",
    "\n",
    "```\n",
    "\n",
    "### 2.b Key points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman filter\n",
    "## Calculating the variances \n",
    "## Applying the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path planning\n",
    "## A* algorithm\n",
    "## Finding keypoints of the path\n",
    "\n",
    "Given the size of the Thymio compared to the size of the grid cells, we won't follow the path cell by cell but instead extract keypoints that will serve as intermediate goals for the Thymio.\n",
    "For cells every ```STEP``` cells, we study the vectors between the previous cell (```STEP``` cells before) and the current cell, and between the current cell and the next cell (```STEP``` cells after).\n",
    "We calculate how much the robot has to turn to change from the first direction to the second one, and if this angle surpasses a threshold (meaning the turn is non negligable), we add the current cell to the list of keypoints of the path. Even if the angles are found insignificant, a cell is added to the keypoints every few steps to make sure we don't ignore too many small changes of direction that could become significant when added together.\n",
    "\n",
    "In this algorithm we chose to study the path in steps of size ```STEP``` instead of considering every single one of the cells because the cells of the grid are very small compared to the Thymio. \n",
    "\n",
    "Using ```math.atan2(det, dot_product)``` allows us to find the signed angle between the two vectors. atan2() can find angles over the whole trigonometric circle (using the information given by the sign of the dot_product), not limiting itself to only two quadrants like atan().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example : Calling ```find_keypoints(path)``` on a random path :\n",
    "\n",
    "```python\n",
    "#parameters :\n",
    "ANGLE_THRESHOLD = math.radians(20)\n",
    "STEP = 2\n",
    "COUNTER_THRESHOLD = 5 #max number of steps between keypoints\n",
    "```\n",
    "\n",
    "<img src=\"./images/path_example.jpg\" alt=\"Diagram\" width=\"200\">\n",
    "\n",
    "TO DO : Explain more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagram showing the thymio, the goal, and the variables used for the controller :\n",
    "\n",
    "<img src=\"./images/schema_astolfi.jpg\" alt=\"Diagram\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to design a controller because it makes the Thymio moves more smoothly than advancing segment by segment and making the robot turn on itself between each segment. Indeed it makes a mix of both the wanted translational and rotational velocities to set the motors' speed :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "v = SPEED                                                   #translational velocity [mm/s]\n",
    "omega = k_alpha*(delta_angle) - k_beta*(delta_angle+theta)  #rotational velocity [rad/s]\n",
    "\n",
    "#Calculate motor speed\n",
    "v_ml = (v+omega*L_AXIS)*SPEED_SCALING_FACTOR #PWM\n",
    "v_mr = (v-omega*L_AXIS)*SPEED_SCALING_FACTOR #PWM\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we implemented an Astolfi controller, but it calculates the translational speed proportionnaly to the distance to the goal, and therefore slows down as it gets closer to the goal. We wanted the thymio to have a constant speed when following his path, and therefore we made some changes to the controller :\n",
    "\n",
    "We start from a baseline translational velocity SPEED, which for each wheel we will adapt depending on delta_angle, to obtain a curved and smooth trajectory.\n",
    "\n",
    "We tuned the controllers parameters by testing. k_alpha controls the rotational velocity, and k_beta is a damping term (to stabilize the robot's orientation when reaching the goal).\n",
    "\n",
    "We obtain motors' speed in rad/s, so before setting it to the thymio we scale it to fit the pwm, limit it (the thymio's motor can accept values ranging from -500 to 500), and convert them to integers (type accepted by the thymio). \n",
    "To find the scaling factor between the robot's speed in mm/s and the motors' target, we used the information given in the thymio cheat sheet : when the motors are set at 500, the translational velocity is equal to about 200mm/s. Therefore we obtain : \n",
    "```python\n",
    "SCALING_FACTOR = 500/200\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local obstacle avoidance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "- There should always be atleast one path from robot to goal, global obstacles cannot completly block the robot and the goal\n",
    "- Local obstacles should be placed in a way such that during local avoidance maneuvres the robot stays inside the arena and doesn't collide with global obstacles\n",
    "- Goal and global obstacles should remain fixed and detectable by their color "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync, aw\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "aw(node.lock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import numpy as np\n",
    "from libs.vision import *\n",
    "from skimage import feature, measure\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "from heapq import heappush, heappop\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Variables initialization\n",
    "Thymio_xytheta_hist = np.empty((0, 3))\n",
    "sigma = 5\n",
    "epsilon = 0.01\n",
    "thresh_obstacle = np.array([[4, 0, 90, 90, 90, 255]])  # Adjust thresholds if needed\n",
    "thresh_goal = np.array([30, 30, 20, 90, 120, 80])\n",
    "min_size = 500\n",
    "grid_size = 300\n",
    "Thymio_detected = False\n",
    "\n",
    "# Uncomment and adjust if using an IP camera\n",
    "\"\"\"\n",
    "print(\"Try to open camera\")\n",
    "\n",
    "login = \"thymio\"\n",
    "password = \"qwertz\"\n",
    "url = f\"https://{login}:{password}@192.168.21.126:8080/video\"  # Check the URL if needed\n",
    "cam = cv2.VideoCapture(url)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Try to open camera\")\n",
    "\n",
    "cam = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "\n",
    "if not cam.isOpened():\n",
    "    print(\"Camera could not be opened\")\n",
    "    cam.release()\n",
    "    exit()\n",
    "\n",
    "print(\"Warming up the camera...\")\n",
    "for _ in range(50):  # Number of frames to discard (adjust if needed)\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame\")\n",
    "        break\n",
    "\n",
    "# Initialization function\n",
    "image, grid, Thymio_xytheta, c_goal, path, obstacle_cnt, obstacle_cnt_expnded, goal_cnt, \\\n",
    "mat_persp, max_width_persp, max_height_persp, aruco_size = init(\n",
    "    cam, sigma, epsilon, thresh_obstacle, thresh_goal, min_size, grid_size, aruco=True)\n",
    "\n",
    "# Convert path coordinates for plotting\n",
    "path_img = grid1_coord2grid2_coord(path, grid, image)\n",
    "path_img = path_img[::-1]\n",
    "\n",
    "image_cnt = draw_cnt_image(\n",
    "    image, goal_cnt, obstacle_cnt, obstacle_cnt_expnded, path_img, Thymio_xytheta, c_goal, aruco_size,Thymio_detected)\n",
    "\n",
    "# Update history\n",
    "Thymio_xytheta_hist = np.vstack((Thymio_xytheta_hist, Thymio_xytheta))\n",
    "\n",
    "print(f\"Initialization time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "######################################################\n",
    "# UPDATE\n",
    "######################################################\n",
    "from motion_control.motion import *\n",
    "\n",
    "for steps in range(500):\n",
    "    start_time = time.time()\n",
    "    image, Thymio_xytheta, Thymio_detected = update_vision(\n",
    "        cam, sigma, epsilon, mat_persp, max_width_persp, max_height_persp)\n",
    "    \n",
    "    #motion_control(adjust_units(Thymio_xytheta, c_goal, aruco_size))\n",
    "\n",
    "    if not Thymio_detected:\n",
    "        Thymio_xytheta = Thymio_xytheta_hist[-1, :]\n",
    "\n",
    "    image_cnt = draw_cnt_image(\n",
    "        image, goal_cnt, obstacle_cnt, obstacle_cnt_expnded, path_img, Thymio_xytheta, c_goal, aruco_size,Thymio_detected)\n",
    "\n",
    "    # Update history\n",
    "    Thymio_xytheta_hist = np.vstack((Thymio_xytheta_hist, Thymio_xytheta))\n",
    "\n",
    "    # Display the image and plots using cv2.imshow\n",
    "    cv2.imshow('Camera View', image_cnt)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    print(f\"Time for this step: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Clean up\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the program\n",
    "aw(node.stop())\n",
    "aw(node.unlock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
